{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07a0692b-cd76-4893-91d5-ab219c74eccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nilearn\n",
    "import nibabel as nib\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13714435-49d9-4f5e-9c92-1450bcb01759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import datasets, image\n",
    "from nilearn.maskers import NiftiMasker\n",
    "from nilearn.plotting import plot_epi, plot_roi, show\n",
    "from nilearn.image import resample_to_img\n",
    "from nilearn.maskers import NiftiLabelsMasker\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2481a63f-1162-4cde-b2d8-a32e076cb5c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e710883-2941-4cf7-9fae-ee5b9b6a60af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The atlas contains 999 non-overlapping regions\n"
     ]
    }
   ],
   "source": [
    "from nilearn.datasets import (load_mni152_template,)\n",
    "mni_template = load_mni152_template(resolution=3)\n",
    "\n",
    "atlas=nilearn.datasets.fetch_atlas_schaefer_2018(n_rois=1000, yeo_networks=17, resolution_mm=1, data_dir=None, base_url=None, resume=True, verbose=1)\n",
    "print(f\"The atlas contains {len(atlas.labels) - 1} non-overlapping regions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dee5df17-8bd2-4b25-928f-54f64feb8769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import ants\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nilearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba78974c-0451-49cd-835b-7d363dca7ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = r'/mnt/k/Abide Dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52511bff-f3a9-4b7f-824d-4df817c09955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_paths(root):\n",
    "    root = os.path.join(root, 'dataset/transfer - Copy - Copy/')\n",
    "    fmri_data_list = {}\n",
    "    universities = os.listdir(root)\n",
    "    for university in universities[:1]:\n",
    "        directory = os.path.join(root, university)\n",
    "    \n",
    "        FolderList = os.listdir(directory)\n",
    "    \n",
    "        for i, patient_id in enumerate(FolderList):\n",
    "            if i == 5:\n",
    "                \n",
    "            rest_file = os.path.join(directory, patient_id, r'session_1/rest_1/rest.nii')\n",
    "            fmri_data_list[patient_id] = rest_file\n",
    "                \n",
    "    return fmri_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d87c372-0052-4fec-8dbd-793737dd48f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = fetch_paths(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0c01b21-cb84-4baa-80fd-0ca81129f673",
   "metadata": {},
   "outputs": [],
   "source": [
    "faulty_subjects = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b38b1874-46ea-454e-9f8a-2604bef08486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    }
   ],
   "source": [
    "print(len(subjects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9fa1a79-6e6a-44a6-9129-3aa72fdda3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_roi(subject_id, fmri_image):\n",
    "    warnings.filterwarnings('ignore', category=UserWarning)\n",
    "    registered_volumes = []\n",
    "    try:\n",
    "        print('trying', subject_id)\n",
    "        for time_idx in tqdm(range(fmri_image.shape[-1]), desc=\"Registering frames\"):\n",
    "            # Extract the 3D volume for the current timestamp\n",
    "            fmri_volume = nib.Nifti1Image(fmri_image.dataobj[..., time_idx], affine=fmri_image.affine)\n",
    "            \n",
    "            # Resample the 3D volume to match the MNI template resolution\n",
    "            resampled_volume = resample_to_img(fmri_volume, mni_template)\n",
    "            \n",
    "            # Convert the resampled volume to an ANTs image for registration\n",
    "            resampled_volume_data = np.array(resampled_volume.dataobj)\n",
    "            resampled_affine = resampled_volume.affine\n",
    "            ants_volume = ants.from_numpy(resampled_volume_data)\n",
    "            ants_volume.set_spacing(tuple(np.sqrt(np.sum(resampled_affine[:3, :3] ** 2, axis=0))))\n",
    "            \n",
    "            # Convert the MNI template to an ANTs image (if not already done)\n",
    "            if time_idx == 0:\n",
    "                mni_template_data = np.array(mni_template.dataobj)\n",
    "                mni_affine = mni_template.affine\n",
    "                ants_mni_template = ants.from_numpy(mni_template_data)\n",
    "                ants_mni_template.set_spacing(tuple(np.sqrt(np.sum(mni_affine[:3, :3] ** 2, axis=0))))\n",
    "            \n",
    "            # Register the current volume to the MNI template\n",
    "            registration_result = ants.registration(fixed=ants_mni_template, moving=ants_volume, type_of_transform='SyN')\n",
    "            \n",
    "            # Append the registered volume to the list\n",
    "            registered_volumes.append(registration_result['warpedmovout'].numpy())\n",
    "        \n",
    "        # Stack all registered volumes along the time axis to create a new 4D image\n",
    "        registered_4d_data = np.stack(registered_volumes, axis=-1)\n",
    "        registered_4d_image = nib.Nifti1Image(registered_4d_data, affine=mni_template.affine)\n",
    "    \n",
    "        masker = NiftiLabelsMasker(\n",
    "        atlas.maps,\n",
    "        labels=labels,\n",
    "        standardize=\"zscore_sample\",\n",
    "        )\n",
    "        masker.fit(registered_4d_image)\n",
    "        signals = masker.transform(registered_4d_image)\n",
    "        signals_df = pd.DataFrame(signals)\n",
    "    \n",
    "        # Save the DataFrame to a CSV file\n",
    "        output_path = f\"{output_dir}/{subject_id}.csv\"  # Adjust the path if needed\n",
    "        signals_df.to_csv(output_path, index=False)\n",
    "\n",
    "        print(\"done for subject\", subject_id)\n",
    "        \n",
    "    except:\n",
    "        faulty_subjects.append(subject_id)\n",
    "        print('Did not run for subject', subject_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22ca676e-f60c-4411-9a9b-6679e49894ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"Unprocessed\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "labels = np.insert(atlas.labels, 0, 'Background')\n",
    "\n",
    "# Step 4: Convert all labels to strings\n",
    "labels = [str(label) for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be5f75aa-a6cb-4515-b6bb-62d9e8682d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subjects_done = os.listdir(output_dir)\n",
    "# left_subjects = {} \n",
    "# for key, val in subjects:\n",
    "#     if os.path.join(key, '.csv') not in list(os.listdir(output_dir)):\n",
    "#         left_subjects[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "101a18ee-5a54-47a8-b557-f128f2a47877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying 29864\n",
      "trying 29865\n",
      "trying 29866\n",
      "trying 29869\n",
      "trying 29868\n",
      "trying 29867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registering frames:   0%|          | 0/160 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying 29870\n",
      "trying 29871\n",
      "trying 29872\n",
      "trying 29873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registering frames:   1%|â–         | 2/160 [00:19<25:27,  9.67s/it]\n",
      "Registering frames:   0%|          | 0/160 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not run for subject 29864\n",
      "trying 29874\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mUserWarning\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Assuming `subjects` is a dictionary where keys are subject IDs and values are file paths\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcal_roi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubject_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubject_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubjects\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-24.08/lib/python3.11/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-24.08/lib/python3.11/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-24.08/lib/python3.11/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "import nibabel as nib\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# Assuming `subjects` is a dictionary where keys are subject IDs and values are file paths\n",
    "results = Parallel(n_jobs=10)(\n",
    "    delayed(cal_roi)(subject_id, nib.load(path)) for subject_id, path in subjects.items()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf867f4-751f-4a34-b633-792e491eb7f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4943bde2-784f-4b6d-9316-84ebf4365745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8824c3-8bf4-4910-9e0b-bac051fcabdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3d00bd-3b63-4779-a2ce-f8a3018669bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
